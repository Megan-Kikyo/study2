Thinking1 奇异值分解SVD的原理是怎样的，都有哪些应用场景
答：任何一个方阵，都可以分解成其（特征向量组成的矩阵，对角矩阵，特征向量组成的矩阵的逆矩阵）三个矩阵的乘积；任何一个m*n的矩阵A，A乘以A的转置为m*m方阵，A的转置乘以A为n*n方阵，得出A的矩阵分解成为（左奇异矩阵，m*m维；对角矩阵，对角线上的非零元素为特征值λ1, λ2, ... , λk；右奇异矩阵，n*n维）三个矩阵的乘积；通过特征值的数量的选择，起到降维的作用。
传统的SVD可应用到推荐算法中去，但因为往往需要原始矩阵是无损的，推荐算法中的原始矩阵往往是稀疏的，需先将原始矩阵进行缺失值补充，造成更大的噪音，推荐结果不精确；因此SVD在实际应用中主要用于降维和图像压缩。

Thinking2 funkSVD, BiasSVD，SVD++算法之间的区别是怎样的
答：funkSVD:避开稀疏问题,原始矩阵约等于两个奇异矩阵的相乘；而且只用两个矩阵进行相乘，损失函数=P和Q矩阵乘积得到的评分，与实际用户评分之差；让损失函数最小化（加入正则项） 从而得到最优解。

BiasSVD：在funkSVD的基础上，考虑到用户自己的偏好，商品自己的偏好，所有记录的整体均值，目标函数变为实际用户评分与（两个矩阵相乘，整体均值，用户自己的偏好，商品自己的偏好）之差；让损失函数最小化（加入正则项）从而得到最优解。

SVD++：在BiasSVD的基础上，考虑了用户隐式反馈信息，使得损失函数最小化，从而得到最优解。

Thinking3 矩阵分解算法在推荐系统中有哪些应用场景，存在哪些不足
答：矩阵分解算法在推荐系统中的应用主要有funkSVD, BiasSVD，SVD++三种算法，分别考虑了用户与商品信息；用户自己的偏好，商品自己的偏好；用户隐式反馈信息。

不足:但因为现实中的原始矩阵往往是稀疏的，需先将原始矩阵进行填充，造成更大的噪音推荐结果不准确；我们都只考虑user和item特征，但实际上一个预测问题包含的特征维度可能很多。

Thinking4 假设一个小说网站，有N部小说，每部小说都有摘要描述。如何针对该网站制定基于内容的推荐系统，即用户看了某部小说后，推荐其他相关的小说。原理和步骤是怎样的

答：Step1，对每部小说描述（Desc）进行特征提取N-Gram，提取N个连续字的集合，作为特征TF-IDF，按照(min_df, max_df)提取关键词，并生成TFIDF矩阵。
Step2，计算小说之间的相似度矩阵余弦相似度。
step3，利用一个用户过去喜欢（不喜欢）的小说的特征数据，来学习该用户的喜好特征，推荐相关性特征最大的Top―K个小说。

Thinking5 Word2Vec的应用场景有哪些
答：Word2Vec 原理：通过Embedding，把原先词所在空间映射到一个新的空间中去，使得语义上相似的单词在该空间内距离相近；学习隐藏层的权重矩阵即word embedding。Word2Vec的两种应用场景：Skip-Gram，给定input word预测上下文；CBOW，给定上下文，预测input word（与Skip-Gram相反）。