Thinking1 参数共享指的是什么？
答：参数共享主要指的是同一个卷积核在不同的图像的位置的权重以及偏差都相同。即隐层的参数个数和隐层的神经元个数无关，只和滤波器的大小和滤波器种类的多少有关。

Thinking2 为什么会用到batch normalization ?
答：batch normalization通过一定的规范化手段，把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布。使得非线性变换函数的输入值落入对输入比较敏感的区域，从而避免梯度消失问题（输入的小变化就会导致损失函数较大的变化，避免梯度消失问题产生）；同时也让收敛速度更快，加快训练速度。

Thinking3 使用dropout可以解决什么问题？
答：使用dropout 主要是为了解决过拟合问题，指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率关闭某些神经网络单位，使之不参与训练，在一定程度上减小过拟合问题，但在一定程度上往往会加大模型的训练时间。
