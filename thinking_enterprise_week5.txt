Thinking1 在CTR点击率预估中，使用GBDT+LR的原理是什么？
答：GBDT:自动发现有效的特征及特征组合,解决了样本量大，人工提取特征成本大的问题;LR:对GBDT产生的输入数据进行分类（使用L1正则化防止过拟合）;使用GBDT+LR，相比单纯的LR和GBDT，在Loss上减少了3%，提升作用明显。
Thinking2 Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）
答：Wide推荐：系统通过获得用户的购物日志数据，包括用户点击哪些商品，购买过哪些商品，然后通过OneHot编码转换为离散特征好处是可解释性强，不足在于特征组合需要人为操作。
Deep推荐：通过深度学习出一些向量，这些向量是隐性特征，往往没有可解释性的。
两个模型分别对全量数据进行预测，然后根据权重组合最终的预测结果joint training：wide和deep的特征合一，构成一个模型进行预测。

Wide部分：学习items或者features之间的相关频率，在历史数据中探索相关性的可行性，从而使模型具有记忆能力。
Deep部分：利用神经网络，基于相关性的传递，通过机器学习，去探索一些在过去没有出现过的特征组合，使模型具有一定的泛化能力。Thinking3 在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？
答：使用FM与DNN结合的方式，主要有两种：串联和并联。
串联：NFM模型，将FM的结果作为DNN的输入。
并联：DeepFM模型和Wide & Deep模型。Wide & Deep模型：Wide模型，采用Linear Regression，解决模型的记忆能力（特征组合需要人来设计）Deep模型，即FNN，解决模型的泛化能力Joint Training，同时训练Wide模型和Deep模型，并将两个模型的结果的加权作为最终的预测结果。DeepFM模型：是将Wide & Deep模型中的Wide替换成了FM模型。
Thinking4 GBDT和随机森林都是基于树的算法，它们有什么区别？
答：（1）GBDT：基于boosting思想；随机森林：基于Bagging思想。
（2）组成随机森林的树可以并行生成，但是组成GBDT的树只能串行生成。
(3)对于最终的输出结果，随机森林采用多数投票；而GBDT是将所有的结果累加起来，或者加权起来


Thinking5 item流行度在推荐系统中有怎样的应用
答：（1）内容的流行程度，也称之为热度，最常见的是将榜单中热度的内容推荐给用户（微博热搜，TopN商品）（2）基于流行度的推荐是围绕流行度计算产生的推荐模型（不仅是TopN）（3）解决冷启动问题 => 根据流行度来推荐商品的算法，也就是什么内容吸引用户，就给用户推荐什么内容。不仅要有代表性和区分性，即不能太大众化或老少皆宜 => 无法区分用户的兴趣；也要具有多样性，用户兴趣的可能性很多，为了匹配兴趣的多样性 => 提供具有较高覆盖率的启动item集合（这些物品能覆盖主流的用户兴趣）。