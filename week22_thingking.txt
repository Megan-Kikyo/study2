Thinking1	机器学习中的监督学习、非监督学习、强化学习有何区别
答：监督学习、无监督学习、强化学习都是机器学习的分支。
监督学习：监督学习有已经准备好的训练数据输出值以及数据特征。
非监督学习：非监督学习中既没有输出值也没有奖励值的，只有数据特征。
强化学习：只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的。

Thinking2	什么是策略网络，价值网络，有何区别？
答：策略网络：任何游戏，玩家的输入被认为是行为a，每个输入（行为）导致一个不同的输出，这些输出称为游戏的状态s可以得到一个不同状态-行动的配对的列表。
价值网络：通过计算目前状态s的累积分数的期望，价值网络给游戏中的状态赋予一个分数（数值），每个状态都经历了整个数值网络。
奖励更多的状态，会在数值网络中的数值Value更大，这里的奖励是奖励期望值，我们会从状态集合中选择最优的状态。

Thinking3	请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的？
答：MCTS是一个搜索算法，它采用的各种方法都是为了有效地减少搜索空间。在MCTS的每一个回合，起始内容是一个半展开的搜索树，目标是原先的半展开+再多展开一个/一层节点的搜索树。
MCTS的作用是通过模拟来进行预测输出结果，理论上可以用于以{state,action}为定义的任何领域。
Step1，选择Select，从根节点往下走，每次都选一个“最有价值的子节点”，直到找到“存在未扩展的子节点”，即这个局面存在未走过的后续着法的节点，比如 3/3 节点
Step2，扩展Expansion，给这个节点加上一个 0/0 子节点，对应之前所说的“未扩展的子节点”
Step3，模拟Simluation，用快速走子策略（Rollout policy）走到底，得到一个胜负结果（Thinking：为什么不采用AlphaGo的策略价值网络走棋）
Step4，回传Backup，把模拟的结果加到它的所有父节点上，假设模拟的结果是 0/1，就把0/1加到所有父节点上 

Thinking4：	假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑？
答：RL考虑的是个体（Agent）与环境（Environment）的交互问题，目标是找到一个最优策略，使Agent获得尽可能多的来自环境的奖励；很多情况下，Agent无法获取全部的环境信息，而是通过观察(Observation)来表示环境（environment），也就是得到的是自身周围的信息。信息流推荐中，用户推荐的视频是否点击是奖励，可考虑用户不同时间段，点击，浏览的次数和时间等行为变量；视频的内容和分类，用户等行为静态变量。

Thinking5：	在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路
答：车辆正常行驶，没有偏离预定轨道，没有撞击障碍物是奖励；具体的路况表示环境，车辆的位置是状态，对车辆的操作是动作，如何造作车辆是策略。
