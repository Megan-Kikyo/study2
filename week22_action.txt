Action（五子棋）：
棋盘大小 10 * 10
采用强化学习（策略价值网络），用AI训练五子棋AI
请说明都有哪些模块，不同模块的原理
答：首先把要解决的问题转化成为一个环境（environment）
状态空间（state space）：对于五子棋来说，每一个棋盘布局（记为s）就是一个状态，所有可能的棋盘布局就是状态空间。
动作空间（action space）：对于五子棋来说，所有可能落子的位置就是一个动作空间。
可行动作（available action）： 给定一个棋盘，哪里可以落子，哪里不可以。
状态转化：下棋之后，对手可能会下的棋。如果是两个Alpha Zero对弈的话，相互是对方环境的一个部分。
奖励函数：下棋之后得到的信号反馈。在五子棋里面，就是胜率的一个正函数。胜率越大，奖励越大。
落子概率也称为策略（policy）
有了落子概率，简单的方式是直接按照这个概率进行落子 =>这会导致神经网络原地踏步，因为Policy Value Network的训练数据是自我对弈（self-play）
仅仅自己学习自己是不会有改进的，需要有一个办法来利用值函数的信息来优化这个策略，可使用蒙特卡洛树搜索（MCTS）来进行策略优化。

